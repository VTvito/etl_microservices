services:
  airflow:
    image: apache/airflow:latest
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    networks:
      - etl_network
    command: > 
      bash -c "airflow db init && airflow webserver & airflow scheduler"
    restart: always

  extraction_service:
    image: extraction_service  # Use the previous built image
    volumes:
      - "C:/Users/vitod/Desktop:/app/data"  # Mount the volume for the service (where is the .cvs test file)
    expose:
      - "5001"
    ports:
      - "5001:5001"
    networks:
      - etl_network
    restart: always

  transformation_service:
    image: transformation_service  # Use the previous built image
    expose:
      - "5002"
    ports:
      - "5002:5002"
    networks:
      - etl_network
    restart: always

networks:   # create a network to allows comunication between services
  etl_network:
    driver: bridge
